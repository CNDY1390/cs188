---
title: 4.5 总结
parent: 4. 马尔可夫决策过程
nav_order: 5
layout: page
header-includes:
    \pagenumbering{gobble}
---

# 4.5 总结 (Summary)

上面介绍的材料很容易让人混淆。我们涵盖了值迭代、策略迭代、策略提取和策略评估，所有这些看起来都很相似，都使用贝尔曼方程，但有细微的变化。

以下是每种算法目的的总结：

-   **值迭代 (Value iteration)**：用于通过迭代更新直到收敛来计算状态的最优值。
-   **策略评估 (Policy evaluation)**：用于计算特定策略下状态的值。
-   **策略提取 (Policy extraction)**：用于在给定某个状态值函数的情况下确定策略。如果状态值是最优的，则此策略将是最优的。此方法用于在运行值迭代后从最优状态值计算最优策略，或作为策略迭代中的子程序来计算当前估计状态值的最佳策略。
-   **策略迭代 (Policy iteration)**：一种封装了策略评估和策略提取的技术，用于迭代收敛到最优策略。由于策略通常比状态值收敛得快得多，因此它往往优于值迭代。
