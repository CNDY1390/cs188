---
title: 5.5 总结
parent: 5. RL
nav_order: 5
layout: page
header-includes:
    \pagenumbering{gobble}
---

# 5.5 总结 (Summary)

非常重要的一点是，强化学习有一个潜在的 MDP，强化学习的目标是通过推导最优策略来解决这个 MDP。使用强化学习与使用值迭代和策略迭代等方法的区别在于，缺乏对潜在 MDP 的转移函数 $$T$$ 和奖励函数 $$R$$ 的了解。因此，代理必须通过在线试错而不是纯粹的离线计算来*学习*最优策略。有很多方法可以做到这一点：

-   **基于模型的学习 (Model-based learning)**：运行计算来估计转移函数 $$T$$ 和奖励函数 $$R$$ 的值，并使用这些估计值使用 MDP 求解方法（如值迭代或策略迭代）。
  
-   **无模型学习 (Model-free learning)**：避免估计 $$T$$ 和 $$R$$，而是使用其他方法直接估计状态的值或 Q-值。
  
  -   **直接评估 (Direct evaluation)**：遵循策略 $$\pi$$ 并简单地计算从每个状态获得的总奖励以及每个状态被访问的总次数。如果采集了足够的样本，这会收敛到 $$\pi$$ 下状态的真实值，尽管速度很慢并且浪费了关于状态之间转移的信息。
  
  -   **时间差分学习 (Temporal difference learning)**：遵循策略 $$\pi$$ 并使用带有采样值的指数移动平均，直到收敛到 $$\pi$$ 下状态的真实值。TD 学习和直接评估是在策略学习的例子，它们在决定该策略是否次优并需要更新之前学习特定策略的值。
  
  -   **Q-学习 (Q-Learning)**：通过带有 Q-值迭代更新的试错直接学习最优策略。这是离策略学习的一个例子，即使采取次优行动也能学习最优策略。
  
  -   **近似 Q-学习 (Approximate Q-Learning)**：做与 Q-学习相同的事情，但使用基于特征的状态表示来概括学习。

-   为了量化不同强化学习算法的性能，我们使用**遗憾 (regret)** 的概念。遗憾捕捉了如果我们从一开始就在环境中表现最优所积累的总奖励与我们通过运行学习算法积累的总奖励之间的差异。
