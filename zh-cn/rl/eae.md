---
title: 5.4 探索与利用
parent: 5. 强化学习
nav_order: 4
layout: page
header-includes:
    \pagenumbering{gobble}
---

# 5.4 探索与利用 (Exploration and Exploitation)

我们现在已经涵盖了代理学习最优策略的几种不同方法，并强调了“充分探索”对此是必要的，但没有真正详细说明“充分”意味着什么。在接下来的部分中，我们将讨论两种在探索和利用之间分配时间的方法：$$\epsilon$$-贪婪策略和探索函数。

## 5.4.1 $$\varepsilon$$-贪婪策略 ($$\varepsilon$$-Greedy Policies)

遵循 **$$\epsilon$$-贪婪策略** 的代理定义某个概率 $$0 \leq \epsilon \leq 1$$，并以概率 $$\epsilon$$ 随机行动和探索。相应地，它们遵循当前建立的策略并以概率 $$(1 - \epsilon)$$ 进行利用。这是一个非常简单的实现策略，但仍然可能很难处理。如果选择了较大的 $$\epsilon$$ 值，那么即使在学习了最优策略之后，代理仍然主要表现为随机行为。同样，选择较小的 $$\epsilon$$ 值意味着代理将不频繁地探索，导致 Q-学习（或任何其他选定的学习算法）非常缓慢地学习最优策略。为了解决这个问题，必须手动调整 $$\epsilon$$ 并随着时间的推移降低它才能看到结果。

## 5.4.2 探索函数 (Exploration Functions)

手动调整 $$\epsilon$$ 的问题可以通过**探索函数 (exploration functions)** 来避免，探索函数使用修改后的 Q-值迭代更新来给予访问较少访问的状态一些偏好。修改后的更新如下：

$$Q(s, a) \leftarrow (1-\alpha)Q(s, a) + \alpha \cdot [R(s, a, s') + \gamma \max_{a'} f(s', a')]$$

其中 $$f$$ 表示探索函数。设计探索函数存在一定程度的灵活性，但常见的选择是使用：

$$f(s, a) = Q(s, a) + \frac{k}{N(s, a)}$$

其中 $$k$$ 是某个预定值，$$N(s, a)$$ 表示 Q-状态 $$(s, a)$$ 被访问的次数。处于状态 $$s$$ 的代理总是从每个状态中选择具有最高 $$f(s, a)$$ 的动作，因此不必在探索和利用之间做出概率决策。相反，探索由探索函数自动编码，因为项 $$\frac{k}{N(s, a)}$$ 可以给一些不常采取的动作足够的“奖励”，使其被选中而不是具有更高 Q-值的动作。随着时间的推移和状态被更频繁地访问，对于每个状态，这个奖励会减少到 $$0$$，并且 $$f(s, a)$$ 回归到 $$Q(s, a)$$，使得利用越来越排他。
